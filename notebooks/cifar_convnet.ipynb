{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models import ResNet18\n",
    "from helpers import get_model_size, estimate_loss, normalize_tensor, CiFaData, get_parameters\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 110\n",
    "BATCH_SIZE = 1024\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we actually just need it to download cifar dataset\n",
    "# torchvision.datasets.CIFAR10(train=True, download=True, root='../data/', transform=transforms.ToTensor())\n",
    "# torchvision.datasets.CIFAR10(train=False, download=True, root='../data/', transform=transforms.ToTensor())\n",
    "\n",
    "tf = transforms.Compose([transforms.RandomCrop(32, padding=4), \n",
    "                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                         transforms.RandomErasing(),\n",
    "                         transforms.RandomRotation(10)])\n",
    "                        #  transforms.ColorJitter(brightness=.2, contrast=.2, saturation=.2),\n",
    "                        #  transforms.RandomAffine(degrees=90)]) \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# create loader to get the params\n",
    "# complete_ds = CiFaData(stage=\"all\", path='../data/')\n",
    "# big_loader = DataLoader(complete_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=14)\n",
    "# params = get_parameters(big_loader)\n",
    "\n",
    "# params = torch.tensor([0.4919, 0.4827, 0.4472]), torch.tensor([0.2470, 0.2434, 0.2616])\n",
    "print(f\"normalized parameters of the dataset: {params}\")\n",
    "\n",
    "train_ds = CiFaData(stage=\"train\", path='../data/', transform=tf, dataset_params=params)\n",
    "val_ds = CiFaData(stage=\"val\", path='../data/', dataset_params=params)\n",
    "test_ds = CiFaData(stage=\"test\", path='../data/', dataset_params=params)\n",
    "\n",
    "# pinning memory, takes cpu data and pins it to the gpu.\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=14, pin_memory=True) \n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=14, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=14, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.init_weights()\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.AdamW(params=[p for p in model.parameters() if p.requires_grad==True], lr=LR)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "# the initial lr is calculated instead of taken from optim: max_lr / div_factor\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer=optimizer, steps_per_epoch=int(train_loader.__len__()),\n",
    "                                    epochs=EPOCHS, max_lr=0.15, anneal_strategy='cos') # , three_phase=True, final_div_factor=1000)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,mode='min', factor=.1, patience=5)\n",
    "# schedule = {\n",
    "#   15: 1e-2,\n",
    "#   60: 1e-3,\n",
    "#   65: 1e-4,\n",
    "#   60: 1e-5\n",
    "# }\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_size = get_model_size(model)\n",
    "\n",
    "# training loop\n",
    "losses = []\n",
    "val_losses = []\n",
    "all_the_lrs = []\n",
    "lrs = []\n",
    "\n",
    "for epoch in (t:=trange(EPOCHS)):\n",
    "  model.train()\n",
    "  running_loss = []\n",
    "  for step, (x, y) in enumerate(train_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    predictions = model(x)\n",
    "    loss = criterion(predictions, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_the_lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step() # with the onecycle scheduler, we step after every batch\n",
    "    running_loss.append(loss.item())\n",
    "\n",
    "  # only one per iteration\n",
    "  losses.append(np.mean(running_loss))\n",
    "  val_loss, val_acc = estimate_loss(model, val_loader, criterion, device)\n",
    "  val_losses.append(val_loss)\n",
    "  # scheduler.step(metrics=val_loss)\n",
    "  # if epoch in schedule.keys():\n",
    "  #   optimizer.param_groups[0]['lr'] = schedule[epoch]\n",
    "  lrs.append(optimizer.param_groups[0]['lr'])\n",
    "  t.set_description(f\"epoch {epoch+1} | training loss: {losses[-1]:.4f} | validation loss: {val_losses[-1]:.6f} | current lr: {lrs[-1]} | validation accuracy: {val_acc:.2f}\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(len(losses)), losses, label='training')\n",
    "ax1.plot(range(len(val_losses)), val_losses, label='validation')\n",
    "ax1.plot(range(len(val_losses)), [np.min(val_losses)]*EPOCHS, color='r', label=f'minimum val loss at epoch {np.argmin(val_losses)+1}')\n",
    "ax1.set_ylabel('loss')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('learning rate')\n",
    "ax2.plot(range(len(lrs)), lrs, color='green', label='learning rate')\n",
    "fig.tight_layout()\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the graph\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"torchlogs/\")\n",
    "writer.add_graph(model, x)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cifar-10-batches-py/batches.meta', 'rb') as f:\n",
    "  meta = pickle.load(f)\n",
    "meta['label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9\n",
    "input_ = x[n].cpu().permute(1,2,0).numpy()\n",
    "\n",
    "plt.title(meta['label_names'][y[n]])\n",
    "plt.imshow(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vizualize filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(weights):\n",
    "  if not weights.shape[1] == 3:\n",
    "    weights = weights[:, 0:1, :, :]\n",
    "    \n",
    "  filter_img = torchvision.utils.make_grid(weights, nrow=int(np.sqrt(weights.shape[0])), normalize=True)\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(filter_img.permute(1,2,0))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[0].prep_block[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[1].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[1].convblock[3].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[2].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[2].convblock[3].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[3].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[2].convblock[3].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[3].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[3].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[3].convblock[3].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[4].convblock[0].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filters(model.resnet[4].convblock[3].weight.detach().cpu().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model_children[0]:\n",
    "  for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature maps: \n",
    "# extract all conv layers\n",
    "weights = []\n",
    "conv_layers = []\n",
    "cnt = 0\n",
    "model_children = list(model.children()) \n",
    "for layer in model_children:\n",
    "  if type(layer) == nn.Conv2d:\n",
    "    cnt += 1\n",
    "    weights.append(layer.weight)\n",
    "    conv_layers.append(layer)\n",
    "  elif type(layer) == nn.Sequential:\n",
    "   for i in range(len(layer)):\n",
    "     for child in layer[i].children():\n",
    "       if type(child) == nn.Conv2d:\n",
    "         cnt += 1\n",
    "         weights.append(child.weight)\n",
    "         conv_layers.append(child)\n",
    "\n",
    "print(f'total conv layers: {cnt}')\n",
    "print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an image, make inference of layer and create feature map\n",
    "image = [x for x,_ in test_loader][0]\n",
    "out = []\n",
    "names = []\n",
    "for layer in conv_layers:\n",
    "  out.append(layer(image))\n",
    "  names.append(str(layer))\n",
    "\n",
    "processed = []\n",
    "for feature_map in out:\n",
    "  feature_map = feature_map.squeeze(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos:\n",
    "## increase size: make a resnet 50\n",
    "### add bottlenecks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcb690d7a096d114b5411ea453cbe6506dfb4402b1a4c8888831b78b4b26966d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
