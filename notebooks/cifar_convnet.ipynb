{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# we actually just need it to download cifar dataset\n",
    "default_ds_train = torchvision.datasets.CIFAR10(train=True, download=True, root='../data/', transform=transforms.ToTensor())\n",
    "default_ds_test = torchvision.datasets.CIFAR10(train=False, download=True, root='../data/', transform=transforms.ToTensor())\n",
    "\n",
    "trainloader = DataLoader(default_ds_train, batch_size=32)\n",
    "testloader = DataLoader(default_ds_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiFaData(Dataset):\n",
    "  def __init__(self, stage=\"train\", transform=None, device=\"cpu\"):\n",
    "    self.device = device\n",
    "    self.base_folder = \"cifar-10-batches-py\"\n",
    "    self.transform = transform\n",
    "    if stage == \"train\":\n",
    "      batch_collection = [f\"data_batch_{i}\" for i in range(1, 5)]\n",
    "    elif stage == \"val\":\n",
    "      batch_collection = [\"data_batch_5\"]\n",
    "    elif stage == \"test\":\n",
    "      batch_collection = [\"test_batch\"]\n",
    "    else:\n",
    "      raise ValueError(\"Invalid stage, choose from train, val, test.\")\n",
    "    self.x_data = []\n",
    "    self.y_data = []\n",
    "    for batch in batch_collection:\n",
    "      with open(f\"../data/cifar-10-batches-py/{batch}\", \"rb\") as f:\n",
    "        data = pickle.load(f, encoding=\"latin1\") \n",
    "        self.x_data.extend(data[\"data\"])\n",
    "        self.y_data.extend(data[\"labels\"])\n",
    "    self.y_data = torch.tensor(self.y_data)\n",
    "    self.x_data = np.vstack(self.x_data).reshape(-1, 3, 32, 32) # from list to np stack; results in (N, 3, 32, 32)\n",
    "    self.x_data = self.x_data.transpose((0, 2, 3, 1)) # into (N, H, W, C)\n",
    "  def __len__(self):\n",
    "    return len(self.y_data)\n",
    "  def __getitem__(self, idx):\n",
    "    if self.transform:\n",
    "      return self.transform(self.x_data[idx]), self.y_data[idx]\n",
    "    return transforms.ToTensor()(self.x_data[idx]).to(self.device), self.y_data[idx].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# tf = transforms.Normalize(0.5, 0.5)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "train_ds = CiFaData(stage=\"train\", device=device)\n",
    "val_ds = CiFaData(stage=\"val\", device=device)\n",
    "test_ds = CiFaData(stage=\"test\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LittleConv(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3,6,5) # out: (B, 6, 28, 28)\n",
    "    self.pool = nn.MaxPool2d(2,2) # (B, 6, 14, 14)\n",
    "    self.fc1 = nn.Linear(6 *14*14, 10) # (B, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.fc1(torch.flatten(x,1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: loss: 1.3500: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'that took 00:13'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "littleconv = LittleConv()\n",
    "optimimizer = optim.SGD(lr=0.001, params=littleconv.parameters(), momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "littleconv.to(device)\n",
    "\n",
    "for i in (t:= trange(epochs)):\n",
    "  for x, y in train_loader:\n",
    "    optimimizer.zero_grad()\n",
    "    pred = littleconv(x)  \n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimimizer.step()\n",
    "\n",
    "  t.set_description(f\"epoch {i+1}: loss: {loss.item():.4f}\")\n",
    "f\"that took {t.format_interval(t.format_dict['elapsed'])} minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "# littleconv = LittleConv()\n",
    "optimimizer = optim.SGD(lr=0.001, params=net.parameters(), momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in (t:= trange(epochs)):\n",
    "  for x, y in train_loader:\n",
    "    optimimizer.zero_grad()\n",
    "    pred = net(x)  \n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimimizer.step()\n",
    "\n",
    "  t.set_description(f\"epoch {i+1}: loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
